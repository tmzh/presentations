<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hitchikers Guide to Large Language Models</title>

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/black.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <link rel="stylesheet" href="style.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css">

    <style>
        .special-header {
            position: absolute;
            padding: 10px 20px 10px 20px;
            top: 30px;
            left: 0px;
            z-index: 500;
            /*background-color: rgba(0,0,0,0.5)*/
        }

        .special-body {
            margin: auto;
            position: absolute;
            top: 50%;
            left: 0%;
            /*-ms-transform: translate(-50%, 50%);*/
            transform: translate(0%, -50%);
        }

        .slides>section {
            width: 100%;
            height: 100%;
        }
    </style>
</head>

<body>

    <div class="reveal">
        <div class="slides">
            <section>
                <h1>Hitchikers Guide to Large Language Models</h1>
            </section>
            <section>
                <section data-auto-animate>
                    <h2>Who is this for?</h2>
                    <aside class="notes">
                        <li>Develop an intuition around 1) how LLMs work, 2) what they can do, and 3) what they can't
                            do.</li>
                        <li>Understand the current state of the art in LLMs and the research directions.</li>
                    </aside>
                </section>

                <section data-auto-animate>
                    <h2>Who is this for?</h2>
                    <img src="images/llm_journey.jpg" alt="LLM Journey" />

                    <aside class="notes">
                        <ul>
                            <li><strong>AI Enthusiasts:</strong> General users of AI applications. Curious about its
                                capabilities and limitations </li>
                            <li><strong>Developers:</strong> Build applications that use the LLMs for various
                                applications such as
                                text
                                generation, chatbots, and more.</li>
                            <li><strong>Researchers:</strong> Focus on architecture, training algorithms, and losses.
                            </li>
                        </ul>
                        <p>- Most academia focuses on architecture, training algorithms, and losses.</p>
                        <p>- Industry emphasizes data, evaluation, and system efficiency.</p>
                        <p>- You may be put off by all these talks on architecture, algorithm etc., but there is a lot
                            of
                            value
                            to be derived from understanding the basics and using in real-life applications.</p>
                    </aside>
                </section>
                <section>
                    <h3>Key Questions about Large Language Models (LLMs)</h3>
                    <ul>
                        <li>Why do LLMs hallucinate?</li>
                        <li>Why does a model generate gibberish at times?</li>
                        <li>Why do people troll LLMs asking them to count R's in strawberry?</li>
                        <li>Are LLMs even do 1+1?</li>
                        <li>Why can't LLMs be trusted to provide source?</li>
                        <li>Why LLMs break when you ask about SolidGoldMagikarp?</li>
                    </ul>

                    </aside>
                </section>

            </section>

            <section>
                <section>
                    <h3>What is a Large Language Model?</h3>
                    <img src="images/how-llms-work.gif" alt="LLM Prediction" />
                    <div class="footer">
                        Source: <a href="https://youtu.be/LPZh9BOjkQs?si=F163sSwIB0xdF7QK">"Large Language Models
                            explained briefly" by 3Blue1Brown</a>
                    </div>
                    <aside class="notes">
                        <ul>
                            <li>LLMs takes in a sequence of words and predict next word<< /li>
                            <li>By doing this 1000s of times, it learns to generate coherent text.</li>
                            <li>Everything from chatbots to coding assistants are based on this principle.</li>
                        </ul>
                    </aside>
                </section>

                <section>
                    <h3>Everything is next word prediction</h3>
                    <div class="r-stack">
                        <img class="fragment fade-out" data-fragment-index="0" src="images/simple_chat_prompt.jpg" />
                        <img class="fragment current-visible" data-fragment-index="0"
                            src="images/coding_assistant.jpg" />
                        <img class="fragment" src="images/image_prompt.jpg" />
                    </div>
                    <aside class="notes">Text generation underpins every application of LLMs, even if it seems unrelated
                        at first</aside>
                </section>

                <section>
                    LLMs are trained on 1000s of books, articles, and websites to produce coherent text.
                </section>

                <section>
                    <div class="r-hstack justify-center" style="height: 400px;">
                        <div class="box fragment" data-fragment-index="2"
                            style="background: #D8BFD8; width: 240px; border-radius: 5px;">LLM</div>
                        <div class="arrow fragment r-fit" data-fragment-index="2"
                            style="width: 150px; text-align: center;">
                            <i class="fas fa-arrow-right"></i>
                        </div>
                        <div class="box fragment" data-fragment-index="1"
                            style="background: #B0E0E6; width: 240px; border-radius: 5px;">Language</div>
                        <div class="arrow fragment r-fit" data-fragment-index="1"
                            style="width: 150px; text-align: center;">
                            <i class="fa fa-arrow-right"></i>
                        </div>
                        <div class="box fragment" data-fragment-index="0"
                            style="background: #60A68C; width: 240px; border-radius: 5px;">World</div>
                    </div>
                    <div class="fragment" data-fragment-index="3">
                        By learning and modelling language, LLMs transitively model the world.
                    </div>
                    <aside class="notes">
                        <li>We know that languages models the world i.e., structure and pattern of the languages mimic
                            the physical truth of real world </li>
                        <li>LLMs captures the structure and patterns of the language, which in turn reflects the
                            structure and patterns of the world.</li>
                    </aside>
                </section>
            </section>


            <section>
                <section>
                    <h3>How to train your own <s>dragon</s> model?</h3>
                    <ul>
                        <li class="fragment">Borrow few 100 million dollars</li>
                        <li class="fragment">Download all of internet (250B pages; >1PB of data)</li>
                        <li class="fragment">Train for 2-3 months on several thousand GPUs</li>
                    </ul>
                    <aside class="notes">
                        <p>Training is a complex process requiring significant infrastructure and investment.</p>
                    </aside>
                </section>
                <section>
                    <h3>Training costs big bucks</h3>
                    <img class="r-stretch" src="images/openai compute cost.jpg" alt="Training Cost" />

                    <aside class="notes">
                        today‚Äôs models cost of order $100 million to train, plus or minus factor two or three
                        The models that are in training now and that will come out at various times later this year or
                        early next year are closer in cost to $1 billion
                        in 2025 and 2026, we‚Äôll get more towards $5 or $10 billion
                    </aside>
                </section>

                <section>
                    <h3>Typical composition of Training data</h3>
                    <img class="r-stretch" src="images/composition-of-pile.png" alt="Composition of Pile" />
                    <aside class="notes">
                        <li>The Pile is one example of a dataset that is used to train LLMs.</li>
                        <li>The Pile is a collection of 22 smaller datasets, each with a different focus. </li>
                    </aside>
                </section>

                <section>
                    <h3>Training data is extensively curated</h3>
                    <img src="images/macrodata-refinement.png" alt="" class="r-stretch">
                </section>

                <section>
                    <h3>What is in a trained model?</h3>
                    <img class="r-stretch" src="images/llm_files.png" alt="LLM Components" />
                    <aside class="notes">
                        <p>Think of it as a zip file compressing the internet.</p>
                    </aside>
                </section>

            </section>

            <section>
                <section>
                    <h3>But Pre-training is Just the Beginning</h3>
                    <img src="images/llama_pretrained.jpg" alt="Pretrained Model" />
                    <aside class="notes">
                        <li>Pre-training only encourages to learn probability distribution of the data</li>
                        <li>It does not guarantee that the model will produce useful or high quality output</li>
                        <li>For example, when asked a question it will produce gibberish after answering. It may keep
                            producing QA pairs simply because it has seen them during training.</li>
                        <li>It is as likely to quote a redditor as it is to quote a scientist</li>
                    </aside>
                </section>

                <section>
                    <h3>How Chat assistants are trained</h3>
                    <img src="images/chat_assistant.excalidraw.png" alt="LLM Alignment" />
                    <aside class="notes">
                        <p>LLMs transition to complex assistant models through SFT and RLHF.</p>
                    </aside>
                </section>

                <section>
                    <h3>Supervised Fine-tuning</h3>
                    <img class="r-stretch" src="images/models_sft.jpg" alt="Models SFT" />
                    <aside class="notes">
                        <li>uses high-quality supervised datasets to train models</li>
                        <li>teaches model to follow instructions and generate more accurate and relevant responses</li>
                        <li>improves model's ability to understand and generate human-like text</li>
                    </aside>
                </section>

                <section>
                    <h3>Reinforcement Learning from Human Feedback (RLHF)</h3>
                    <img src="images/rhlf.jpg" alt="RLHF" />

                    <div class="footer">
                        source: <a href="https://arxiv.org/pdf/2309.00267">RLAIF vs. RLHF: Scaling Reinforcement
                            Learning
                            from Human Feedback with AI Feedback</a>
                    </div>
                    <aside class="notes">
                        <li>Reinforcement Learning from Human Feedback focuses on maximizing human preference rather
                            than merely replicating human responses.</li>
                        <li>Humans find it much easier to compare two responses than to evaluate a single response.
                        </li>
                        <li>It uses comparison labels either generated by humans or AI</li>
                        <li>RLHF uses a reward model to compare responses and a policy model to generate responses.
                        <li>encourages model to generate more coherent, relevant, and safe responses.</li>
                    </aside>
                </section>
            </section>


            <section>
                <section>
                    <h2>Under the hood</h2>
                </section>
                <section>
                    <h2>How next token prediction works?</h2>
                    <div class="r-stack">
                        <img class="fragment" src="frames/img0001.png" alt="frame 1">
                        <img class="fragment" src="frames/img0002.png" alt="frame 2">
                        <img class="fragment" src="frames/img0003.png" alt="frame 3">
                        <img class="fragment" src="frames/img0004.png" alt="frame 4">
                        <img class="fragment" src="frames/img0005.png" alt="frame 5">
                        <img class="fragment" src="frames/img0006.png" alt="frame 6">
                        <img class="fragment" src="frames/img0007.png" alt="frame 7">
                        <img class="fragment" src="frames/img0008.png" alt="frame 8">
                        <img class="fragment" src="frames/img0009.png" alt="frame 9">
                        <img class="fragment" src="frames/img0010.png" alt="frame 10">
                        <img class="fragment" src="frames/img0011.png" alt="frame 11">
                        <img class="fragment" src="frames/img0012.png" alt="frame 12">
                        <img class="fragment" src="frames/img0013.png" alt="frame 13">
                        <img class="fragment" src="frames/img0014.png" alt="frame 14">
                        <img class="fragment" src="frames/img0015.png" alt="frame 15">
                        <img class="fragment" src="frames/img0016.png" alt="frame 16">
                        <img class="fragment" src="frames/img0017.png" alt="frame 17">
                        <img class="fragment" src="frames/img0018.png" alt="frame 18">
                        <img class="fragment" src="frames/img0019.png" alt="frame 19">
                        <img class="fragment" src="frames/img0020.png" alt="frame 20">
                        <img class="fragment" src="frames/img0021.png" alt="frame 21">
                        <img class="fragment" src="frames/img0022.png" alt="frame 22">
                        <img class="fragment" src="frames/img0023.png" alt="frame 23">
                        <img class="fragment" src="frames/img0024.png" alt="frame 24">
                        <img class="fragment" src="frames/img0025.png" alt="frame 25">
                        <img class="fragment" src="frames/img0026.png" alt="frame 26">
                        <img class="fragment" src="frames/img0027.png" alt="frame 27">
                        <img class="fragment" src="frames/img0028.png" alt="frame 28">
                        <img class="fragment" src="frames/img0029.png" alt="frame 29">
                    </div>
                    <div class="footer">
                        source: <a
                            href="https://lena-voita.github.io/nlp_course/language_modeling.html">https://lena-voita.github.io/nlp_course/language_modeling.html</a>
                    </div>
                </section>
                <section>
                    <h2>Controlling the output of LLMs</h2>
                </section>
                <section>
                    <h4>Temperature</h3>
                        <img src="images/temperature.png" alt="Temperature Control" />
                        <aside class="notes">
                            <li>Higher temperature increases randomness in predictions.</li>
                            <li>Temperatures works by 1/T scaling the logits before softmax.</li>
                        </aside>
                </section>

                <section>
                    <h4>Top-k sampling</h4>
                    <img src="images/topk_problems-min.png" alt="Top-k Sampling" />
                    <aside class="notes">
                        <p>Top-k sampling can cover small parts of the probability mass.</p>
                    </aside>
                </section>

                <section>
                    <h4>Top-p sampling</h4>
                    <img src="images/nucleus_example-min.png" alt="Top-p Sampling" />
                </section>

                <section>
                    <h2>Understanding Tokens</h2>
                </section>
                <section>
                    <p class="fragment" data-fragment-index="0">Tokens are the smallest units of text recognized by
                        models.</p>
                    <p class="fragment" data-fragment-index="1"><strong>1000 tokens ‚âà 750 words</strong></p>
                    <p class="fragment" data-fragment-index="2">Models can only process a fixed context length measured
                        in tokens.</p>
                    <aside class="notes"> Tokens are leant using algorithms like Byte Pair Encoding (BPE) separately
                        from a single pass over the data even before model training begins</aside>
                </section>

                <section>
                    <h3>Splitting text into tokens</h3>
                    <div class="header-underline"> It is raining </div>
                    <p class="fragment" data-fragment-index="0">Character level tokenization</p>
                    <div class="token-row">
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #F0E68C;">I
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #FFB6C1;">t
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #D8BFD8;">
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #B0E0E6;">i
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #FFB6C1;">s
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #D8BFD8;">
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #B0E0E6;">r
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #F0E68C;">a
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #B0E0E6;">i
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #FFB6C1;">n
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #B0E0E6;">i
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #FFB6C1;">n
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #F0E68C;">g
                        </div>
                        <div class="fragment token-box" data-fragment-index="1" style="background-color: #D8BFD8;">.
                        </div>
                    </div>
                    <p class="fragment" data-fragment-index="4">Sub-word level tokenization</p>
                    <div class="token-row">
                        <div class="fragment token-box word" data-fragment-index="5" style="background-color: #F0E68C;">
                            It
                        </div>
                        <div class="fragment token-box word" data-fragment-index="5" style="background-color: #FFB6C1;">
                            is
                        </div>
                        <div class="fragment token-box word" data-fragment-index="5" style="background-color: #D8BFD8;">
                            rain
                        </div>
                        <div class="fragment token-box word" data-fragment-index="5" style="background-color: #B0E0E6;">
                            ing
                        </div>
                        <div class="fragment token-box word" data-fragment-index="5" style="background-color: #F0E68C;">
                            .
                        </div>
                    </div>
                    <p class="fragment" data-fragment-index="2">Word level tokenization</p>
                    <div class="token-row">
                        <div class="fragment token-box word" data-fragment-index="3" style="background-color: #F0E68C;">
                            It
                        </div>
                        <div class="fragment token-box word" data-fragment-index="3" style="background-color: #FFB6C1;">
                            is
                        </div>
                        <div class="fragment token-box word" data-fragment-index="3" style="background-color: #D8BFD8;">
                            raining</div>
                        <div class="fragment token-box word" data-fragment-index="3" style="background-color: #B0E0E6;">
                            .
                        </div>
                    </div>
                    <aside class="notes">
                        <ul>
                            <li>Treating every character as a token limits the amount of information that can be
                                handled within a given context.</li>
                            <li>Word level tokenization can fit more information into a context than character level
                                tokenization, as it can handle longer words and phrases. However, it can't handle
                                out-of-vocabulary words.</li>
                            <li>Word level tokenization also increases vocabulary size. As the size of
                                the vocabulary increases, the embedding weights and model size also increase.</li>
                            <li>Subword tokenization: Balances context and vocabulary size; handles complex language.
                                Uses algorithms like BPE.</li>
                            <li>When to split a word? BPE algorithm preserves subwords that occur
                                frequently in the training data</li>
                            <li>Typical sub-word vocabulary size: 50,000 - 250,000 tokens.</li>
                            </li>
                        </ul>
                    </aside>
                </section>

                <section>
                    <h3>Putting them all together</h3>
                    <a
                        href="https://poloclub.github.io/transformer-explainer/">https://poloclub.github.io/transformer-explainer/</a>

                </section>

                <section>
                    <h3>Re-examining Key Questions about Large Language Models</h3>
                    <ul>
                        <li class="fragment">Why do LLMs hallucinate?</li>
                        <li class="fragment">Why does a model generate gibberish at times?</li>
                        <li class="fragment">Why do people troll LLMs asking them to count R's in strawberry?</li>
                        <li class="fragment">Are LLMs good at 1+1?</li>
                        <li class="fragment">Why LLMs break when you ask about SolidGoldMagikarp?</li>
                        <li class="fragment">Why can't LLMs be trusted to provide source?</li>
                    </ul>
                    <aside class="notes">
                        <ul>
                            <li>
                                <b>Hallucination:</b>
                                <ul>
                                    <li>
                                        <p>LLMs predict the next token probabilistically. While training encourages
                                            relevant outputs, the inherent exploration of novel relationships makes some
                                            level of hallucination unavoidable, especially with inconsistent data.</p>
                                    </li>
                                    <li>
                                        <p>At one end of the spectrum, consider search engines. They take the prompt and
                                            return one of the most similar "training documents" verbatim from their
                                            database. This search engine exhibits a "creativity problem" - it will never
                                            respond with something new.</p>
                                    </li>
                                    <li>
                                        <p>An LLM is 100% creative and faces the hallucination problem. A search engine
                                            is 0% creative and encounters the creativity problem.</p>
                                    </li>
                                </ul>
                            </li>
                            <li><b>Gibberish:</b> Multiple factors contribute to gibberish: using a weak base model,
                                inappropriate quantization (distorting token relationships), or "magic words" (unique
                                tokens removed from training, disrupting learned relationships). These issues make all
                                output tokens equally likely.</li>
                            <li><b>Strawberry:</b> LLMs lack the ability to decompose and understand tokens
                                as sequence of characters. They struggle with character level questions unless training
                                data has explicit examples in suffiicient quantity.</li>
                            <li><b>Number Handling:</b> LLMs treat numbers as categorical tokens. Common numbers might
                                have dedicated tokens, while uncommon ones are split, lacking inherent ordinal
                                relationships. While LLMs *can* learn ordinality, it requires extensive training and
                                consistent tokenization, which is often lacking. Expect inaccuracies with uncommon
                                numbers.</li>
                            <li><b>Source Attribution:</b> Tokenization breaks text into sub-word units. Accurate source
                                reproduction requires an exact token sequence. The stochastic nature of generation means
                                a single incorrect token can derail the entire reference.</li>
                        </ul>
                </section>
            </section>


            <section>
                <section>
                    <h2>How to use Large Language Models</h2>
                </section>

                <section>
                    <h3>Proprietary Models</h3>
                    <h4>Model Properties</h4>
                    <img src="images/openai_models.jpg" alt="Model Selection" />
                </section>


                <section>
                    <h3>Proprietary Models</h3>
                    <h4>Runtime Configurable Parameters</h4>
                    <pre><code data-line-numbers="|10|11|12|">from openai import OpenAI
    client = OpenAI()
    
    completion = client.chat.completions.create(
      model="gpt-3.5-turbo",
      messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
      ],
      temperature=0.7,
      top_p=0.95, # use top_p or temp, not both
      top_logprobs=3,
      max_tokens=1000,
    )
    </code>
</pre>
                </section>

                <section>
                    <h3>Open Source Models</h3>
                    <h4>Model Properties</h4>
                    <pre><code data-line-numbers="|12|24|">{
      "architectures": [
        "MistralForCausalLM"
      ],
      "attention_dropout": 0.0,
      "bos_token_id": 1,
      "eos_token_id": 2,
      "hidden_act": "silu",
      "hidden_size": 4096,
      "initializer_range": 0.02,
      "intermediate_size": 14336,
      "max_position_embeddings": 32768,
      "model_type": "mistral",
      "num_attention_heads": 32,
      "num_hidden_layers": 32,
      "num_key_value_heads": 8,
      "rms_norm_eps": 1e-05,
      "rope_theta": 1000000.0,
      "sliding_window": null,
      "tie_word_embeddings": false,
      "torch_dtype": "bfloat16",
      "transformers_version": "4.42.0.dev0",
      "use_cache": true,
      "vocab_size": 32768
    }</code>
</pre>
                </section>

                <section>
                    <h3>Open Source Models</h3>
                    <h4>Runtime Configurable Parameters</h4>
                    <pre><code class="language-python" data-line-numbers="|13|14|15|">from transformers import AutoTokenizer, AutoModelForCausalLM
    
    tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
    model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")
    
    inputs = tokenizer("Hello, my name is", return_tensors="pt")
    
    # Similar settings to OpenAI's API
    outputs = model.generate(
        **inputs,
        max_length=1000,
        do_sample=True,
        temperature=0.7,
        top_p=0.95,
        top_k=0,
        num_return_sequences=1,
    )
    </code></pre>
                </section>
            </section>

            <section>
                <section>
                    <h1>Addressing LLM Limitations</h1>
                </section>

                <section>
                    <h3>Characteristics of LLMs</h3>
                    <br>
                    <ul style="list-style-type: none;">
                        <li><span class="fragment">üîÑ Predicts next token</span><span class="fragment">
                                stochastically</span></li>
                        <li><span class="fragment">üß© Coherent</span><span class="fragment"> within fixed context
                                length.</span></li>
                        <li><span class="fragment">üìñ Built-in Knowledge </span><span class="fragment"> with a cut-off
                                date</span></li>
                    </ul>
                </section>

                <section>
                    <img class="r-stretch" src="images/hallucination_meme.jpg" alt="Hallucination Example" />
                    <p>Working with LLMs can be a frustrating experience</p>
                    <aside class="notes">
                        <p>People actually mean they don't want an LLM Assistant to hallucinate.</p>
                        <p>An LLM Assistant is more than just the LLM although LLM is at the heart of it; it's a complex
                            system.</p>
                        <p>It's the responsibility of
                            the developer to tailor safety measures to their specific use case and to ensure that the
                            model's outputs are safe and appropriate for the context.
                        </p>

                    </aside>
                </section>
                <section>
                    <h3>Addressing LLM Limitations</h3>
                    <img src="images/addressing_llm_limitations.jpg" alt="Addressing LLM Limitations" />
                    <aside class="notes">
                        <ul>There are many ways to address LLM limitations. I will walk through a few</ul>
                        <ul>RAG: Retriever-augmented generation</ul>
                        <ul>Tool Use: Supplement LLMs with knowledge from external tools</ul>
                        <ul>Prompt Engineering: Guide the model to produce more accurate results</ul>
                    </aside>
                </section>

                <section>
                    <h3>Retriever-augmented generation (RAG)</h3>
                    <img class="r-stretch" src="images/rag.png" alt="RAG">
                    <aside class="notes">
                        RAG works around hallucination and knowledge cutoff limitations by grounding the model's
                        responses in data retrieved from a knowledgebase.
                    </aside>
                </section>

                <section>
                    <h3>Tool Use</h3>
                    <img class="r-stretch" src="images/function-calling-diagram.png" alt="Function Calling">
                    <aside class="notes">
                        Just like RAG, supplement LLMs with knowledge from external tools. Circumvents knowledge
                        cut-off,
                        abstract reasoning limitations.
                    </aside>
                </section>

                <section>
                    <h3>Deliberate problem solving using thoughts</h3>
                    <img class="r-stretch" src="images/llm_thinking.jpg" alt="CoT">
                    <aside class="notes">
                        <li>LLMs are encouraged to generate, search and evaluate thoughts as a coherent language
                            sequence that serves as an intermediate step toward problem solving.</li>
                        <li>Tradeoff is that it increases the length of the prompt and the response.</li>
                        <li>This is the method used in O1 models announced by Open AI.</li>
                        <li>Yesterday Qwen released a 32B model which is said to compete with O1</li>
                    </aside>
                </section>




            </section>

            <section>
                <section>
                    <h1>Choosing the Right Large Language Model (LLM)</h1>
                </section>

                <section>
                    <h3>Different Kinds of Models</h3>
                    <div class="r-hstack">
                        <div class="fragment fade-left div-box">
                            <strong>Pre-trained Models</strong>
                            <ul>
                                <li>e.g., Llama 3, GPT Base</li>
                            </ul>
                        </div>
                        <div class="fragment fade-left div-box">
                            <strong>Instruct-Tuned Variants</strong>
                            <ul>
                                <li>e.g., GPT-3.5-turbo, Llama-3.2-3B-Instruct</li>
                            </ul>
                        </div>
                    </div>
                    <div class="r-hstack">
                        <div class="fragment fade-left div-box">
                            <strong>Coding Assistants</strong>
                            <ul>
                                <li>Codestral, CodeLlama, Davinci Codex etc.</li>
                            </ul>
                        </div>
                        <div class="fragment fade-left div-box">
                            <strong>Mixture-of-Experts</strong>
                            <ul>
                                <li>e.g., GPT-4, Mixtral</li>
                            </ul>
                        </div>
                    </div>
                    <aside class="notes">
                        <ul>
                            <li class="fragment fade-in-then-out"><strong>Pre-trained Models:</strong> Foundation models
                                trained on massive datasets (e.g., Llama 3, GPT Base). These are general-purpose but may
                                require further fine-tuning.</li>
                            <li class="fragment fade-in-then-out"><strong>Instruction-Tuned Models:</strong> Pre-trained
                                models further trained on instructions, leading to better adherence to user prompts
                                (e.g., GPT-3.5-turbo, Llama-3.2-3B-Instruct).</li>
                            <li class="fragment fade-in-then-out"><strong>Specialized Models:</strong> Models tailored
                                for specific tasks like code generation (e.g., Codestral, CodeLlama, Davinci Codex)</li>
                            <li>Mixture-of-Experts: Models that combine the strengths of multiple smaller models,
                                allowing for more specialized and efficient task handling (e.g., GPT-4, Mixtral).</li>
                        </ul>
                    </aside>
                </section>

                <section>
                    <p>Evaluating LLMs is complex and multifaceted. There's no single perfect metric.</p>
                    <ul>
                        <li class="fragment fade-in-then-out"><strong>Benchmarks:</strong> Task-specific, tricky broader
                            applicability</li>
                        <li class="fragment fade-in-then-out"><strong>Human Evaluation:</strong> Inconsistent,
                            challenging consensus
                        </li>
                        <li class="fragment fade-in-then-out"><strong>Bot Evaluation:</strong> Not always aligned with
                            human
                            judgement
                        </li>
                    </ul>
                    <aside class="notes">
                        <ul>
                            <li><strong>Benchmarks:</strong> Task-specific tests offer
                                quantitative comparisons, but may not fully capture real-world performance.</li>
                            <li><strong>Human Evaluation:</strong> Essential for
                                assessing
                                aspects like fluency, coherence, and factual accuracy, but prone to subjectivity and
                                inconsistency.</li>
                            <li>Why not use bot evaluation? Bots may not always align with human judgement, and they can
                                be influenced by biases and limitations.</li>
                        </ul>

                    </aside>
                </section>
                <section>
                    <h3>LM Chatbot Arena</h3>
                    <img src="images/lmsys_chatbot_arena.jpg" alt="LM Chatbot Arena">
                    <div>üèÜ Elo rating system for model ranking</div>
                    <aside class="notes">
                        <ul>
                            <li>ü§ñ Comparative evaluation platform for language models</li>
                            <li>üë• Crowdsourced human judgments</li>
                            <li>üîÑ Continuous updates with new models</li>
                            <li>üîç Insights into model strengths and weaknesses</li>
                            <li>provides a dynamic, user-driven approach to LLM evaluation </li>
                        </ul>
                    </aside>
                </section>

                <section>
                    <h3>Aider LLM Leaderboards</h3>
                    <img src="images/aider_code_editing_benchmark.jpg" alt="LM Chatbot Arena">
                    <div>Coding tasks benchmkarks</div>
                    <aside class="notes">
                        <ul>
                            <li>Coding task specific benchmark</li>
                            <li>Separate rankings for different aspects such as refactoring, editing</li>
                        </ul>
                    </aside>
                </section>

                <section>
                    <h3> Berkeley Function-Calling Leaderboard </h3>
                    <img src="images/function_calling_benchmark.jpg" alt="LM Chatbot Arena">
                    <div>Function calling benchmkarks</div>
                    <aside class="notes">
                        <li>Evaluates function calling at multiple complexities</li>
                        <li>Single Turn: In a single-turn interaction, there is exactly one exchange between the user
                            and the assistant. The user sends a single input, and the assistant responds with a single
                            output.</li>
                        <li>Multi-Step: Multi-step refers to an interaction where the assistant performs several
                            internal function calls to address a single user request. The result of one call may serve
                            as input for subsequent calls.</li>
                        <li>Multi-Turn: Multi-turn interaction involves multiple exchanges between the user and the
                            assistant. Each turn can contain multiple steps, allowing for more complex workflows across
                            several interactions.</li>

                        </ul>
                    </aside>
                </section>

                <section>
                    <h2>Practical Considerations</h2>
                </section>

                <section>
                    <h3>1 Licensing and Legal</h3>
                    <ul>
                        <li class="fragment"><strong>Commercial Services</strong>
                            <ul>
                                <li>Regional availability</li>
                                <li>Data Protection Compliance</li>
                            </ul>
                        </li>
                        <li class="fragment"><strong>Open Weights</strong>
                            <ul>
                                <li>Availability ‚â† free use</li>
                                <li>Verify licensing restrictions</li>
                            </ul>
                        </li>
                    </ul>
                </section>

                <section>
                    <h3>2 Cost and Resource Requirements</h3>
                </section>

                <section>
                    <h4>Model size</h4>
                    <img class="r-stretch" src="images/size_vs_throughput.jpg" alt="Model Size">
                    <p>More Params -> Expensive & Slower</p>
                    <aside class="notes">
                        Model Size and Throughput: Larger models generally offer better performance but demand more
                        computational resources, impacting cost and inference speed. This can be visualized using the
                        image `size_vs_throughput.jpg`.

                    </aside>
                </section>
                <section>
                    <h4>Cost vs Performance</h4>
                    <img src="images/llm_cost.jpg" alt="">
                    <div>For common tasks, most models would do OK.</div>
                    <aside class="notes">
                        For many common tasks, the performance difference between models might not justify the
                        significant cost increase.
                    </aside>
                </section>

                <section>
                    <h4>Context Length</h4>
                    <img src="images/context_sizes.jpg" alt="Context Length">
                    Models with larger context windows can handle longer conversations and docuemnts.
                    <aside class="notes">
                        "Context Length: The amount of text a model can process at once. Longer context windows are
                        beneficial for handling lengthy documents or extended conversations."

                    </aside>
                </section>

                <section>
                    <h4>Self-Hosting</h4>
                    <img src="images/self-hosting-roi.jpg" alt="Self-Hosting">
                    <p>Upfront Investment in GPUs ‚â† Cost Savings</p>
                    <aside class="notes">
                        <li>Significant upfront investment in hardware (GPUs) is required, accompanied by
                            maintenance. Cost savings are not guaranteed.</li>
                        <li>Nvidia, literally pitched to their investors & datacenter customers, in their 2023 investor
                            presentation - the ‚Äúmarket opportunity‚Äù on renting H100s at $4/hr</li>
                        <li>For most of 2023, the H100 prices felt like they would forever be above $4.70 (unless you
                            were willing to do a huge upfront downpayment)</li>
                        <li>At the start of 2024, the H100 prices reached approximately $2.85 across multiple providers.
                        </li>
                        <li>In Aug 2024, if you're willing to auction for a small slice of H100 time (days to weeks),
                            you can start finding H100 GPUs for $1 to $2 an hour.</li>
                    </aside>
                </section>

            </section>
            <section>
                <section>
                    <h2>The current state-of-play in AI</h2>
                    <img src="images/closed_vs_open.png" alt="">
                    <aside class="notes">
                        <li>OpenAI had a year+ lead in creating a GPT-4 class model.</li>
                        <li>Now there are four GPT-4 class models.</li>
                        <li>Llama 3.2 closes the gap between closed-source and open-weight models</li>
                        <li>Small 8B models are as performant as GPT3 when they came out</li>
                    </aside>
                </section>

                <section>
                    <h2>Open AI</h2>
                    <div style="text-align: left;">
                        <strong>Models:</strong>
                        <ul>
                            <li>Pre-trained & Instruct-tuned: GPT3, GPT4, GPT 3.5 Turbo, GPT 4 Turbo</li>
                            <li>Multi-modal & generation: GPT4 Vision, GPT4o, GPT4o-mini, Whisper, Suno, DALL-E 3
                            </li>
                            <li>Reasoning: o1, o1-mini</li>
                        </ul>
                        <strong>Notable features:</strong>
                        <ul>
                            <li>ü§ù Strategic investment from Microsoft</li>
                            <li>üîß Fine-tuning API offered</li>
                            <li>üìû Function calling, üìä JSON mode, üíæ Context-caching (since Oct 2024)
                            </li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>Anthropic</h2>
                    <div style="text-align: left;">
                        <strong>Models:</strong>
                        <div>
                            <ul>
                                <li>Claude 3.0 Haiku, Sonnet, Opus</li>
                                <li>Claude 3.5 Sonnet</li>
                            </ul>
                        </div>
                        <strong>Key features:</strong>
                        <ul>
                            <li>üë§ Founded by Dario Amodei (ex-VP of OpenAI)</li>
                            <li>‚ö° Sonnet 3.5: Top performing model, comparable performance to GPT-4o</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>Google</h2>
                    <div style="text-align: left;">
                        <strong>Models:</strong>
                        <ul>
                            <li>Open Models: Gemma-1 (2B, 7B), Gemma-2 (2B, 9B, 27B)</li>
                            <li>Proprietary Models: Gemini 1.5 (128B), Gemini 2 (540B)</li>
                        </ul>
                        <strong>Key features:</strong>
                        <ul>
                            <li>Gemma: Lightweight, strong open models; pre-trained on 2T-13T English tokens</li>
                            <li>Gemini: Long context (2M tokens) & pioneered context caching</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>Alibaba Cloud</h2>
                    <div style="text-align: left;">
                        <strong>Model Series:</strong>
                        <ul>
                            <li>Qwen, Qwen1.5, Qwen2, Qwen2.5 (0.5B - 110B)</li>
                        </ul>
                        <strong>Qwen2.5 Highlights:</strong>
                        <ul>
                            <li>üìö 18T token pre-training</li>
                            <li>üí° Enhanced knowledge, coding & math skills</li>
                            <li>üìÑ 8K+ token generation & structured data handling</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>Meta</h2>
                    <div style="text-align: left;">
                        <strong>Model Series:</strong>
                        <ul>
                            <li>LLaMA: 7B-65B; LLaMA 2: 7B-70B; LLaMA 3/3.1/3.2: 8B-405B</li>
                        </ul>
                        <strong>LLaMA 3.2 Highlights:</strong>
                        <ul>
                            <li>üèÜ Competitive with GPT-4, GPT-4o, Claude 3.5 Sonnet</li>
                            <li>üìä 405B params, 15T tokens, 128K context</li>
                            <li>üíª Training scale-up: 2,048 A100 (65B) ‚Üí 16,000+ H100 (405B)</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>Mistral</h2>
                    <div style="text-align: left;">
                        <strong>Models:</strong>
                        <ul>
                            <li>Open Models: Mistral (7B), Mistral Nemo (12B), Pixtral (22B), Codestral (22B)</li>
                            <li>Proprietary Models: Large 2 (123B), Mixtral (8√ó7B, 8√ó22B)</li>
                        </ul>
                        <strong>Key features:</strong>
                        <ul>
                            <li>üá´üá∑ French startup, Apache 2 licensed models</li>
                            <li>Proprietary models on Le Platform; fine-tuning API offered</li>
                            <li>Mistral 7B outperforms larger models; Large 2 matches top models like GPT-4o, Llama
                                3
                                405B</li>

                        </ul>
                    </div>
                </section>

                <section>
                    <h3>Putting them all together</h3>
                    <a href="https://llm-stats.com/">https://llm-stats.com/</a>

                </section>
            </section>

            <section>
                <section>
                    <h1>Future Directions in GenAI</h1>
                </section>

                <section>
                    <h2>Shrinking foundation model creation market</h2>
                    <img src="images/shrinking_foundation_models.jpg" alt="" class="r-stretch">
                </section>

                <section>
                    <h2>The Four wars of the AI Stack</h2>
                    <div class="r-stack">
                        <img class="fragment current-visible" data-fragment-index="0" src="images/data_quality_war.jpg"
                            height="450" />
                        <img class="fragment current-visible" data-fragment-index="1" src="images/compute_power_war.jpg"
                            height="450" />
                        <img class="fragment current-visible" data-fragment-index="2"
                            src="images/multi_modality_war.jpg" height="450" />
                        <img class="fragment" src="images/ops_war.jpg" height="450" />
                    </div>
                    <aside class="notes">


                        <b>Data War:</b>
                        <li>A battle for control of high-quality data, pitting content creators against AI models that
                            seek to consume their work.</li>
                        <li>OpenAI partners with Axel Springer; also deals with AP and Data Partnerships.</li>
                        <li>NYT lawsuit against OpenAI to halt GPT operations.</li>
                        <li>Apple offers $50M for data contracts with publishers.</li>
                        <li>Growing interest in synthetic data at NeurIPS and DeepMind.</li>

                        <b>GPU/Inference War:</b>
                        <li>A conflict between those with access to powerful GPUs for AI training and inference, and
                            those without, highlighting the uneven distribution of computational resources.</li>
                        <li>Mixtral output token prices drop from ~$2 to $0.27 in a week.</li>
                        <li>Benchmark competition between Anyscale and other inference providers.</li>
                        <li>Research on new architectures (Mamba, RWKV) emerging.</li>
                        <li>Shifting compute off Nvidia with Modular, tinycorp, and Apple MLX.</li>

                        <b>Multimodality War:</b>
                        <li>A struggle for dominance in AI model capabilities, with specialist models competing against
                            general-purpose models that aim to handle multiple data types.</li>
                        <li>Midjourney soft-launches v6, reports >$200M/year revenue.</li>
                        <li>Suno AI exits stealth mode; steady enhancements in point solutions.</li>
                        <li>OpenAI and Google advance God Models to compete broadly.</li>

                        <b>RAG/Ops War:</b>
                        <li>A fight over the infrastructure and tools needed to build and deploy reliable AI
                            applications, focusing on databases, frameworks, and operational processes </li>
                        <li>Debate on the necessity of Vector DBs; power users adopt new options like turbopuffer.</li>
                        <li>Competition between LangChain (v0.1) and LlamaIndex (with Step-Wise Agent Execution).</li>
                        <li>Ongoing LLMOps innovations (HumanLoop‚Äôs .prompt file, Openlayer) vs. framework-driven tools
                            like LangSmith.</li>
                    </aside>
                </section>

                <section>
                    <h2>GPUs go brrrr</h2>
                    <img class="r-stretch" src="images/gpus_go_brrr.jpg" alt="Scaling Laws">
                    <p>Simpleüìà: More compute = better models</p>
                    <aside class="notes">Rich Sutton's "bitter lesson" exposes a fundamental conflict in AI: the
                        academic preference for elegant, clever algorithms versus the reality that brute-force
                        approaches (massive data and compute) consistently win. This explains the frustration
                        surrounding models like GPT-3, which easily surpass decades of research focused on clever
                        solutions.</aside>
                </section>
            </section>
        </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/zoom/zoom.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/search/search.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>
        // More info about initialization & config:
        // - https://revealjs.com/initialization/
        Reveal.initialize({
            controls: true,
            progress: true,
            center: true,
            hash: true,

            // Learn about plugins: https://revealjs.com/plugins/
            plugins: [RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight]
        });
    </script>

</body>